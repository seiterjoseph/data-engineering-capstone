{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Immigration data and city temperature data provided by Udacity is going to be used to create a star schema that allow users to see if there is a relation between City Temperature and immigration requests to this City. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The scope of the project is to provide an analytical schema for users using provided immigration dataset and temperature dataset so user can use the generated model to find answer for questions like is there a relationship between city temperature and number of immigrant? Do people from countries with warmer or cold climate immigrate to the US in large numbers?\n",
    "\n",
    "The country dimension table is made up of data from the global land temperatures by city and the immigration datasets. The combination of these two datasets allows analysts to study correlations between global land temperatures and immigration patterns to the US and get insights into migration patterns into the US based on the temperature, demographics as well as overall population of states. We could also ask another questions such as, do populous states attract more visitors on a monthly basis?\n",
    "\n",
    "#### Describe and Gather Data \n",
    "##### I94 Immigration Data:\n",
    "This dataset comes from the US National Tourism and Trade Office. it is provided as SAS files by Udacity.\n",
    "\n",
    "###### main fields in the data set are\n",
    "- i94yr   : 4 digit year\n",
    "- i94mon  : numeric month\n",
    "- i94cit  : code of origin city\n",
    "- i94port : code of destination city\n",
    "- arrdate : arrival date\n",
    "- i94mode : travel code\n",
    "- depdate : departure date\n",
    "- i94visa : Visa codes collapsed into three categories Business, Pleasure and Student\n",
    "- i94bir  : Age of Respondent in Years\n",
    "\n",
    "##### World Temperature Data: \n",
    "This dataset came from Kaggle and provided as CSV file by Udacity.\n",
    "this dataset has the follwoing fields AverageTemperature, City, Country, Latitude and Longitude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_sep16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_nov16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_mar16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_may16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_jan16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_jul16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_feb16_sub.sas7bdat',\n",
       " '../../data/18-83510-I94-Data-2016/i94_dec16_sub.sas7bdat']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first get all SAS data files and store them in list\n",
    "#os.path.join(os.getcwd(), 'data/18-83510-I94-Data-2016')\n",
    "#os.listdir('../../data/18-83510-I94-Data-2016')\n",
    "files_list = [os.path.join('../../', 'data/18-83510-I94-Data-2016', f) for f in os.listdir('../../data/18-83510-I94-Data-2016')]\n",
    "files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "# get one file of the file list and work on it\n",
    "fname = files_list[0] \n",
    "#fname #'../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Temperature_File = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_Temp = pd.read_csv(Temperature_File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://96c2a62f31b2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8d3acdcda0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to parquet\n",
    "#df_spark.write.parquet(\"sas_data\")\n",
    "#df_spark=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate',\n",
       "       'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'count',\n",
       "       'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd', 'entdepu',\n",
       "       'matflag', 'biryear', 'dtaddto', 'gender', 'insnum', 'airline',\n",
       "       'admnum', 'fltno', 'visatype'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see all columns in the immigration df\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['XXX', 'ATL', 'WAS', 'NYC', 'TOR', 'BOS', 'HOU', 'MIA', 'CHI',\n",
       "       'LOS', 'CLT', 'DEN', 'DAL', 'DET', 'NEW', 'FTL', 'LVG', 'ORL',\n",
       "       'NOL', 'PIT', 'SFR', 'SPM', 'POO', 'PHI', 'SEA', 'SLC', 'TAM',\n",
       "       'HAM', 'NAS', 'VCV', 'MAA', 'AUS', 'HHW', 'OGG', 'PHO', 'SDP',\n",
       "       'SFB', 'EDA', 'MON', 'CLG', 'DUB', 'FMY', 'YGF', 'SAJ', 'CIN',\n",
       "       'BAL', 'RDU', 'WPB', 'STT', 'OAK', 'NSV', 'SNA', 'OTT', 'X96',\n",
       "       '5KE', 'CLE', 'HAR', 'PSP', 'CHR', 'HAL', 'SAA', 'KOA', 'SHA',\n",
       "       'WIN', 'BGM', 'NCA', 'OPF', 'SAI', 'JFA', 'AGA', 'ONT', 'CLM',\n",
       "       'STL', 'W55', 'CHS', 'SNJ', 'SRQ', 'ANC', 'LNB', 'LIH', 'MIL',\n",
       "       'INP', 'KAN', 'ROC', 'SAC', 'BRO', 'LAR', 'RNO', 'SGR', 'ELP',\n",
       "       'MCA', 'MDT', 'SPE', 'FPR', 'SYR', 'ICT', 'MLB', 'ADS', 'TUC',\n",
       "       'DLR', 'CAE', 'CHA', 'HSV', 'WIL', 'HPN', 'HEF', 'BRG', 'BED',\n",
       "       'DAB', 'JAC', 'FRB', 'SWF', 'KEY', 'PTK', 'MWH', 'X44', 'MYR',\n",
       "       'APF', 'ATW', 'PVD', 'BUF', 'PIE', 'MHT', 'BDL', 'NYL', 'VNY',\n",
       "       '5T6', 'LEX', 'NOR', 'BQN', 'MEM', 'INT', 'CRQ', 'SPO', 'FOK',\n",
       "       'PEV', 'FAR', 'MAF', 'TKI', 'OMA', 'LOU', 'PHF', 'RST', 'MMU',\n",
       "       'CPX', 'SCH', 'RYY', 'PEM', 'JKM', 'LYN', 'OGD', 'NC8', 'MOB',\n",
       "       'SAV', 'HIG', 'CHL', 'WLL', 'MTH', 'AXB', 'SUM', 'ADW', 'SGJ',\n",
       "       'JMZ', 'BLA', 'SSM', 'YIP', 'EPI', 'GSP', 'BHX', 'MND', 'FCA',\n",
       "       'CRP', 'YHC', 'PHU', 'COB', 'OTM', 'STR', 'PSM', 'FWA', 'SYS',\n",
       "       'PEN', 'ABQ', 'HEL', 'DPA', 'CHM', 'EGP', 'POR', 'PIR', 'ORO',\n",
       "       'LUK', 'DER', 'DOU', 'SWE', 'NOO', 'LAN', 'VIC', 'COO', 'NRN',\n",
       "       'REN', 'HTM', 'HID', 'BEL', 'CLS', 'PRO', 'PRE', 'PCF', 'RIF',\n",
       "       'ROS', 'PAR', 'BEE', 'DAC', 'NOG', 'BTN', 'DNS', 'NAC', 'GAL',\n",
       "       'FPT', 'ABG', 'MAS', 'PTL', 'TEC', 'ROO', 'BAU', 'FRI', 'TRO',\n",
       "       'ANA', 'FRE', 'AND', 'LEW', 'NIA', 'PBB', 'THO', 'YSL', 'BEB',\n",
       "       'DLB', 'DNA', 'FTC', 'GPM', 'LAU', 'LCB', 'LLB', 'MOO', 'NEC',\n",
       "       'PHR', 'ROU', 'SKA', 'WHO', 'ANZ', 'BOA', 'CAL', 'MAD', 'MOR',\n",
       "       'ROM', 'WBE', 'AGN', 'CNA', 'SLU', 'FER', 'ALC', 'MET', 'PDN',\n",
       "       'PNH', 'VIB', 'WAL', 'BWA', 'CHT', 'DVL', 'FRT', 'HNN', 'HNS',\n",
       "       'HVR', 'SJO', 'WAR', 'COL', 'TUR', 'ABS', 'BWM', 'CNC', 'RAY',\n",
       "       'VCB', 'MGM', 'MRC', 'PGR', 'LOI', 'ADT', 'NRG', 'CRY', 'ERC',\n",
       "       'FTF', 'FTK', 'SHR', 'MAI', 'NIG', 'NRT', 'VNB', 'FAL', 'LUB',\n",
       "       'RIO', 'LWT'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique i94port codes \n",
    "df['i94port'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  37.,   25.,   55.,   28.,    4.,   57.,   63.,   46.,   48.,\n",
       "         52.,   33.,   58.,   56.,   62.,   49.,   43.,   53.,   74.,\n",
       "         65.,   35.,   32.,   38.,   68.,   61.,   41.,   45.,   54.,\n",
       "         29.,   42.,   34.,   47.,   64.,   27.,   59.,   60.,   66.,\n",
       "         51.,   22.,   39.,   20.,   50.,   44.,   40.,   31.,   23.,\n",
       "         36.,    2.,    0.,   70.,   26.,   30.,   16.,   14.,   21.,\n",
       "         24.,    1.,   77.,   73.,   71.,    6.,   72.,    5.,   76.,\n",
       "         69.,   67.,    3.,   10.,   18.,   19.,   11.,   17.,    9.,\n",
       "          8.,   12.,   75.,    7.,   13.,   15.,   82.,   84.,   78.,\n",
       "         81.,   87.,   79.,   80.,   83.,   91.,   85.,   86.,   88.,\n",
       "         90.,   89.,   97.,   96.,   93.,   92.,  100.,   95.,   98.,\n",
       "         94.,   99.,   nan,  109.,  108.,  107.,  101.,  105.,  102.,\n",
       "        103.,   -3.,  114.,  110.,  111.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get unique values in i94bir column\n",
    "df['i94bir'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see maximum value of i94bir column \n",
    "max(df['i94bir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see minimum value of i94bir column \n",
    "min(df['i94bir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### we can clean immigration data by the following\n",
    "- drop rows for invalid destination cities (i94port column), we can get the vaild city code from description file (lines 303 to 893)\n",
    "- drop rows with invalid 194bir like -3 and nan , the values should be between 1 and 115\n",
    "- keep relevant columns only as we have 28 columns and we do not need all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get valid i94port codes and cities only and save it as dictionary of {code: list}\n",
    "# i copied the codes from SAS Description file and save it as new file and removed invalid values so it is easy for me to read it\n",
    "\n",
    "#with open('I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "#    valid_lines = f.readlines()[302:893]\n",
    "\n",
    "with open('valid_ports_me.txt') as f:\n",
    "    valid_lines = f.readlines()\n",
    "    \n",
    "re_obj = re.compile(r'\\'(.*)\\'.*\\'(.*)\\'')\n",
    "valid_ports = {}\n",
    "for line in valid_lines:\n",
    "    match = re_obj.search(line)\n",
    "    valid_ports[match.group(1)]=[match.group(2)]\n",
    "\n",
    "#valid_ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "now we can start to clean immigrtion data as we have valid i94port codes\n",
    "this function take file path and do the following\n",
    "1- read the file as spark dataframe\n",
    "2- drop rows with invalid values in i94port column\n",
    "3- drop rows with i94bir value not between 1 and 116 years\n",
    "4- select the desired columns only from the whole 28 columns\n",
    "'''\n",
    "def get_Immigration_Spark_df_from_SAS_file(file_path):\n",
    "    '''\n",
    "    Input: File Path for SAS File to be loaded\n",
    "    \n",
    "    Output: Cleansed and ready to use Spark DataFrame with Immigration Data\n",
    "    \n",
    "    '''\n",
    "    spark_df = spark.read.format('com.github.saurfang.sas.spark').load(files_list[0])\n",
    "    spark_df = spark_df.filter(spark_df.i94port.isin(list(valid_ports.keys())))\n",
    "    spark_df = spark_df.filter(spark_df.i94bir.between(1,116))\n",
    "    spark_df = spark_df.select(['i94yr', 'i94mon', 'i94cit', 'i94port', 'i94mode', 'i94bir', 'arrdate', 'depdate', 'i94visa'])\n",
    "    return spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function with one file\n",
    "immigration_df = get_Immigration_Spark_df_from_SAS_file(files_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------+-------+------+-------+-------+-------+\n",
      "| i94yr|i94mon|i94cit|i94port|i94mode|i94bir|arrdate|depdate|i94visa|\n",
      "+------+------+------+-------+-------+------+-------+-------+-------+\n",
      "|2016.0|   4.0| 254.0|    ATL|    1.0|  25.0|20551.0|   null|    3.0|\n",
      "|2016.0|   4.0| 101.0|    WAS|    1.0|  55.0|20545.0|20691.0|    2.0|\n",
      "|2016.0|   4.0| 101.0|    NYC|    1.0|  28.0|20545.0|20567.0|    2.0|\n",
      "|2016.0|   4.0| 101.0|    NYC|    1.0|   4.0|20545.0|20567.0|    2.0|\n",
      "|2016.0|   4.0| 101.0|    NYC|    1.0|  57.0|20545.0|20555.0|    1.0|\n",
      "+------+------+------+-------+-------+------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display the first 5 rows in the generated df\n",
    "immigration_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temperature Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the first 5 rows in df\n",
    "df_Temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dt', 'AverageTemperature', 'AverageTemperatureUncertainty', 'City',\n",
       "       'Country', 'Latitude', 'Longitude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8599212, 7)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3490, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Temp.drop_duplicates(['City', 'Country']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Århus', 'Çorlu', 'Çorum', ..., 'Zurich', 'Zuwarah', 'Zwolle'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Temp['City'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8599212</td>\n",
       "      <td>8.235082e+06</td>\n",
       "      <td>8.235082e+06</td>\n",
       "      <td>8599212</td>\n",
       "      <td>8599212</td>\n",
       "      <td>8599212</td>\n",
       "      <td>8599212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3239</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3448</td>\n",
       "      <td>159</td>\n",
       "      <td>73</td>\n",
       "      <td>1227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1959-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>India</td>\n",
       "      <td>36.17N</td>\n",
       "      <td>139.23E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9545</td>\n",
       "      <td>1014906</td>\n",
       "      <td>425455</td>\n",
       "      <td>129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.672743e+01</td>\n",
       "      <td>1.028575e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.035344e+01</td>\n",
       "      <td>1.129733e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.270400e+01</td>\n",
       "      <td>3.400000e-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.029900e+01</td>\n",
       "      <td>3.370000e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.883100e+01</td>\n",
       "      <td>5.910000e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.521000e+01</td>\n",
       "      <td>1.349000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.965100e+01</td>\n",
       "      <td>1.539600e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                dt  AverageTemperature  AverageTemperatureUncertainty  \\\n",
       "count      8599212        8.235082e+06                   8.235082e+06   \n",
       "unique        3239                 NaN                            NaN   \n",
       "top     1959-08-01                 NaN                            NaN   \n",
       "freq          3510                 NaN                            NaN   \n",
       "mean           NaN        1.672743e+01                   1.028575e+00   \n",
       "std            NaN        1.035344e+01                   1.129733e+00   \n",
       "min            NaN       -4.270400e+01                   3.400000e-02   \n",
       "25%            NaN        1.029900e+01                   3.370000e-01   \n",
       "50%            NaN        1.883100e+01                   5.910000e-01   \n",
       "75%            NaN        2.521000e+01                   1.349000e+00   \n",
       "max            NaN        3.965100e+01                   1.539600e+01   \n",
       "\n",
       "               City  Country Latitude Longitude  \n",
       "count       8599212  8599212  8599212   8599212  \n",
       "unique         3448      159       73      1227  \n",
       "top     Springfield    India   36.17N   139.23E  \n",
       "freq           9545  1014906   425455    129600  \n",
       "mean            NaN      NaN      NaN       NaN  \n",
       "std             NaN      NaN      NaN       NaN  \n",
       "min             NaN      NaN      NaN       NaN  \n",
       "25%             NaN      NaN      NaN       NaN  \n",
       "50%             NaN      NaN      NaN       NaN  \n",
       "75%             NaN      NaN      NaN       NaN  \n",
       "max             NaN      NaN      NaN       NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Temp.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.650999999999996\n",
      "-42.70399999999999\n"
     ]
    }
   ],
   "source": [
    "# display max and min values in AverageTemperature column to see if there is any outliers\n",
    "print(max(df_Temp['AverageTemperature']))\n",
    "print(min(df_Temp['AverageTemperature']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "it seems that the average temerpature is valid but we need to remove duplicate rows and rows with Nan and we need to add i94port code as a \n",
    "new column to temperature data set so we can join it with immigration data set we achive this by creation a user defined function that take the \n",
    "city name and return i94code from valid codes dictionary we built above\n",
    "'''\n",
    "\n",
    "@udf()\n",
    "def get_i94port_Code_By_City(city_name):\n",
    "    '''\n",
    "    Input: City name    \n",
    "    Output: the corresponding i94port value if founded    \n",
    "    '''\n",
    "    \n",
    "    for k in valid_ports:\n",
    "        if city_name.upper() in valid_ports[k][0].upper():\n",
    "            return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "now we will build the function that take Temperature data file path and read it as spark data frame\n",
    "then clean it and return to us a clean and ready to use Spark DataFrame\n",
    "1- read CSV File as Spark DataFrame\n",
    "2- drop duplicates rows by city and country\n",
    "3- drop rows with AverageTemperature = NaN\n",
    "4- drop rows with i94port = null\n",
    "5- add new column i94port by lookup city name in valid ports dictionary\n",
    "6- return the DataFrame\n",
    "'''\n",
    "def get_Temperature_spark_df_from_CSV_file(file_path):\n",
    "    '''\n",
    "        Input: file path that contains Temperature Data (csv file)\n",
    "        Output: clean and ready to use Spark DataFrame with Temperature Data\n",
    "    '''\n",
    "    temperature_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n",
    "    temperature_df = temperature_df.filter(temperature_df.AverageTemperature != 'NaN')\n",
    "    temperature_df = temperature_df.dropDuplicates(['City', 'Country'])\n",
    "    temperature_df = temperature_df.withColumn(\"i94port\", get_i94port_Code_By_City(temperature_df.City))\n",
    "    temperature_df = temperature_df.filter(temperature_df.i94port != 'null')\n",
    "    return temperature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+--------+-------------+--------+---------+-------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|    City|      Country|Latitude|Longitude|i94port|\n",
      "+----------+------------------+-----------------------------+--------+-------------+--------+---------+-------+\n",
      "|1852-07-01|            15.488|                        1.395|   Perth|    Australia|  31.35S|  114.97E|    PER|\n",
      "|1828-01-01|            -1.977|                        2.551| Seattle|United States|  47.42N|  121.97W|    SEA|\n",
      "|1743-11-01|             2.767|                        1.905|Hamilton|       Canada|  42.59N|   80.73W|    HAM|\n",
      "|1849-01-01| 7.399999999999999|                        2.699| Ontario|United States|  34.56N|  116.76W|    ONT|\n",
      "|1821-11-01|             2.322|                        2.375| Spokane|United States|  47.42N|  117.24W|    SPO|\n",
      "+----------+------------------+-----------------------------+--------+-------------+--------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the function and display the first 5 rows\n",
    "temperature_df = get_Temperature_spark_df_from_CSV_file(Temperature_File)\n",
    "temperature_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "we will create a star schema with the two dimensions tables and on fact table because the star schema is widly used and easy\n",
    "to understand by users\n",
    "\n",
    "- Dimension Tables\n",
    "    - Demographic_Dim: has the following columns and populated from the I94 immigration data set.\n",
    "        - I94YR   : year\n",
    "        - I94MON  : month\n",
    "        - I94CIT  : origion city code\n",
    "        - I94PORT : destionation city code\n",
    "        - I94MODE : travel code\n",
    "        - I94BIR  : Age of Respondent in Years between 1 and 116\n",
    "        - ARRDATE : Arrival date\n",
    "        - DEPDATE : Departure date\n",
    "        - I94VISA : Visa type (Business/Pleasure/Student)\n",
    "\n",
    "    - Temperature_Dim: has the following columns and populated from the temperature dataset.\n",
    "        - AverageTemperature : Average Temperature\n",
    "        - City               : City Name\n",
    "        - Country            : Country Name \n",
    "        - Latitude           : Latitude\n",
    "        - Longitude          : Longitude\n",
    "        - I94PORT            : city code mapped from valid codes in SAS Description File\n",
    "\n",
    "- Immigration_Fact: has the following columns populated from immigration data set and temperature dataset\n",
    "    - year              : year (I94YR)\n",
    "    - month             : month (I94MON)\n",
    "    - origion_city      : origion city code (I94CIT)\n",
    "    - destionation_city : destionation city code (I94PORT)\n",
    "    - travel_code   \t: travel code (I94MODE)\n",
    "    - respondent_age\t: Age of Respondent in Years between 1 and 116 (I94BIR)\n",
    "    - arrival_date\t\t: Arrival date (ARRDATE)\n",
    "    - departure_date\t: Departure date (DEPDATE)\n",
    "    - visa_type\t\t\t: Visa type (Business/Pleasure/Student) (I94VISA)\n",
    "    - avgTemp \t\t\t: Average Temperature\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "- load immigration sas files and clean it using function get_Immigration_Spark_df_from_SAS_file wich return clean and ready to use spark DataFrame\n",
    "- write this spark dataframe as parquet file partitiond by i9port(destination city)\n",
    "\n",
    "- load Temperature CSV file and clean it using function get_Temperature_spark_df_from_CSV_file wich return clean and ready to use spark DataFram\n",
    "- write this spark dataframe as parquet file partitiond by i9port(destination city)\n",
    "\n",
    "- create fact table by joining the above two DataFrames and write to parquet file partition by i94port(destination city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load immigration data\n",
    "immigration_df = get_Immigration_Spark_df_from_SAS_file(files_list[0])\n",
    "# wirite data to parquet file\n",
    "immigration_df.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"DWH/immigration_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load temperature data\n",
    "temperature_df = get_Temperature_spark_df_from_CSV_file(Temperature_File)\n",
    "# wirite data to parquet file\n",
    "temperature_df.write.mode(\"append\").partitionBy(\"i94port\").parquet(\"DWH/temperature_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary views of the two data frames to create fact table \n",
    "immigration_df.createOrReplaceTempView(\"immi_view\")\n",
    "temperature_df.createOrReplaceTempView(\"temp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the fact table by joining the immigration and temperature views\n",
    "fact_table = spark.sql(\"\"\"\n",
    "SELECT i.i94yr as year,\n",
    "       i.i94mon as month,\n",
    "       i.i94cit as orgin_City,\n",
    "       i.i94port as destination_city,\n",
    "       i.i94mode as travel_code,\n",
    "       i.i94bir as respondent_age,\n",
    "       i.arrdate as arrival_date,\n",
    "       i.depdate as depature_date,\n",
    "       i.i94visa as visa_type,\n",
    "       t.AverageTemperature as avgTemp\n",
    "FROM immi_view i\n",
    "JOIN temp_view t ON (i.i94port = t.i94port)\n",
    "\"\"\")\n",
    "\n",
    "# Write fact table to parquet files partitioned by i94port\n",
    "fact_table.write.mode(\"append\").partitionBy(\"destination_city\").parquet(\"DWH/fact.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if df has data or not\n",
    "def count_quality_check(df):\n",
    "    '''\n",
    "    Input: Spark DataFrame\n",
    "    Output: 0 for Success and -1 for Failed\n",
    "    '''\n",
    "    if df.count() == 0:\n",
    "        print('Count Quality Check Failed, Empty DataFrame')\n",
    "        return -1\n",
    "    else:\n",
    "        print('Count Quality Check Successed')\n",
    "        return 0\n",
    "\n",
    "# check integrity between immigration and temperature dataframes\n",
    "def integrity_quality_check(df_immigration, df_temp):\n",
    "    '''\n",
    "    Input: two Spark DataFrames\n",
    "    Output: 0 for Success and -1 for Failed\n",
    "    '''\n",
    "    if df_immigration.select(\"i94port\").distinct().join(df_temp, df_immigration[\"i94port\"] == df_temp[\"i94port\"], \"left_anti\")\\\n",
    "    .count() == 0:\n",
    "        print('Integrity Quality Check Failed')\n",
    "        return -1\n",
    "    else:\n",
    "        print('Integrity Quality Check Successed')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "if count_quality_check(immigration_df)  == 0 and count_quality_check(temperature_df) == 0 \\\n",
    "and integrity_quality_check(immigration_df,temperature_df) == 0:\n",
    "    print('Success')\n",
    "else:\n",
    "    print('Failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "- Dimension Tables\n",
    "    - Demographic_Dim: has the following columns and populated from the I94 immigration data set.\n",
    "        - I94YR   : 4 digits year\n",
    "        - I94MON  : numeric month\n",
    "        - I94CIT  : origion city code\n",
    "        - I94PORT : destionation city code\n",
    "        - I94MODE : travel code\n",
    "        - I94BIR  : Age of Respondent in Years between 1 and 116\n",
    "        - ARRDATE : Arrival date\n",
    "        - DEPDATE : Departure date\n",
    "        - I94VISA : Visa type (Business/Pleasure/Student)\n",
    "\n",
    "    - Temperature_Dim: has the following columns and populated from the temperature dataset.\n",
    "        - AverageTemperature : Average Temperature\n",
    "        - City               : City Name\n",
    "        - Country            : Country Name \n",
    "        - Latitude           : Latitude\n",
    "        - Longitude          : Longitude\n",
    "        - I94PORT            : destionation city code mapped from valid codes in SAS Description File.\n",
    "\n",
    "- Immigration_Fact: has the following columns populated by joining the two dim tables\n",
    "    - year              : year (I94YR)\n",
    "    - month\t\t\t\t: month (I94MON)\n",
    "    - origion_city \t\t: origion city code (I94CIT)\n",
    "    - destionation_city : destionation city code (I94PORT)\n",
    "    - travel_code   \t: travel code (I94MODE)\n",
    "    - respondent_age\t: Age of Respondent in Years between 1 and 116 (I94BIR)\n",
    "    - arrival_date\t\t: Arrival date (ARRDATE)\n",
    "    - departure_date\t: Departure date (DEPDATE)\n",
    "    - visa_type\t\t\t: Visa type (Business/Pleasure/Student) (I94VISA)\n",
    "    - avgTemp \t\t\t: Average Temperature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "  - For exploring the data i used panda libirary as it is easy to use, then for processing data i prefered to use Spark as it can handly many files with large amount of data and varies data formats. also spark sql used to perform standard SQL like operations on the data like joining the two dataframes to generate the fact table\n",
    "\n",
    "  - We used star schema model not only it is the most common modeling paradigm but because it tends to be better for performance and it has a small number of tables and clear join paths, queries run faster than they do against an OLTP system. Small single-table queries, usually of dimension tables, are almost instantaneous.\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "since the data arrive monthly in files so we do not have the complexity of defining delta so we can load data every month when the data files arrive to our side and append to the previous data\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "     - we can increase the work power of the spark cluster (maybe using autoscaling service here will be very helpful, when we got the files and start our batch job the cluster will increase and after that it will shrink) or we can change to Amazon Redshift DWH as it is scalable and can handle large amount of data\n",
    "     \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    - we can use a tool like Airflow to schedule the batch job to run daily and configure the success and fail scenario\n",
    "    \n",
    " * The database needed to be accessed by 100+ people.\n",
    "     - with this new settings we can go for Redshift Database as it is very scalable and can handle this number of requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
